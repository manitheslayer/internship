{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adac3919",
   "metadata": {},
   "source": [
    "# 1. Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a563742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad1af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to display all header tags of wiki\n",
    "\n",
    "def wiki(page):\n",
    "    link = requests.get(page)\n",
    "    print(link)\n",
    "    bs = BeautifulSoup(link.content)\n",
    "    result= bs.find_all([\"h1\",\"h2\",\"h3\",\"h4\"])\n",
    "    print(result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361ae7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the webpage: https://en.wikipedia.org/wiki/Main_Page\n",
      "<Response [200]>\n",
      "[<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\">Main Page</h1>, <h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>, <h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>, <h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>, <h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>, <h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>, <h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>, <h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>, <h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>, <h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>, <h2>Navigation menu</h2>, <h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>]\n"
     ]
    }
   ],
   "source": [
    "# asking url as parameter from user\n",
    "# the link is https://en.wikipedia.org/wiki/Main_Page\n",
    "a = input(\"enter the webpage: \")\n",
    "# calling the wiki function\n",
    "wiki(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97bfa7",
   "metadata": {},
   "source": [
    "# 2 . Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b8954be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "117b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to display IMBD top rated 100 movies\n",
    "\n",
    "def imdb(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    print(page)\n",
    "\n",
    "    bs = BeautifulSoup(page.content)\n",
    "\n",
    "    name = []\n",
    "    for i in bs.find_all(\"h3\",class_=\"lister-item-header\"):\n",
    "        name.append(i.text.split(\"\\n\")[2])\n",
    "        \n",
    "        \n",
    "        \n",
    "    year = []\n",
    "    for i in bs.find_all(\"span\",class_=\"lister-item-year text-muted unbold\"):\n",
    "        year.append(i.text.replace(\"(\",\"\").replace(\")\",\"\").replace(\"I\",\"\"))\n",
    "        \n",
    "        \n",
    "    rating=[]\n",
    "    for i in bs.find_all(\"div\",class_=\"ratings-bar\"):\n",
    "        rating.append(i.text.split(\"\\n\")[3])\n",
    "        \n",
    "        \n",
    "    output= pd.DataFrame({\"Movie Name\":name,\"Year of Release\":year,\"Rating\":rating})\n",
    "    return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab9908a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the IMDB URL: https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&count=100\n",
      "<Response [200]>\n",
      "                                       Movie Name Year of Release Rating\n",
      "0                        The Shawshank Redemption            1994    9.3\n",
      "1                                   The Godfather            1972    9.2\n",
      "2                                 The Dark Knight            2008    9.1\n",
      "3   The Lord of the Rings: The Return of the King            2003    9.0\n",
      "4                                Schindler's List            1993    9.0\n",
      "..                                            ...             ...    ...\n",
      "95                             North by Northwest            1959    8.3\n",
      "96                                        Vertigo            1958    8.3\n",
      "97                            Singin' in the Rain            1952    8.3\n",
      "98                                   Citizen Kane            1941    8.3\n",
      "99              M - Eine Stadt sucht einen Mörder            1931    8.3\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# getting input\n",
    "# the link is https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&count=100\n",
    "\n",
    "url = input(\"Enter the IMDB URL: \")\n",
    "a = imdb(url)\n",
    "\n",
    "# printing the output\n",
    "print(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efff993",
   "metadata": {},
   "source": [
    "# 3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7d72ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5276e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to display IMBD top rated 100 Indian movies\n",
    "\n",
    "def imdb(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    print(page)\n",
    "\n",
    "    bs = BeautifulSoup(page.content)\n",
    "\n",
    "    name = []\n",
    "    for i in bs.find_all(\"h3\",class_=\"lister-item-header\"):\n",
    "        name.append(i.text.split(\"\\n\")[2])\n",
    "        \n",
    "        \n",
    "        \n",
    "    year = []\n",
    "    for i in bs.find_all(\"span\",class_=\"lister-item-year text-muted unbold\"):\n",
    "        year.append(i.text.replace(\"(\",\"\").replace(\")\",\"\").replace(\"I\",\"\"))\n",
    "        \n",
    "        \n",
    "    rating=[]\n",
    "    for i in bs.find_all(\"div\",class_=\"ipl-rating-star small\"):\n",
    "        rating.append(i.text.replace(\"\\n\",\"\"))\n",
    "        \n",
    "        \n",
    "    output= pd.DataFrame({\"Movie Name\":name,\"Year of Release\":year,\"Rating\":rating})\n",
    "    return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "add79040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the IMDB URL: https://www.imdb.com/list/ls056092300/\n",
      "<Response [200]>\n",
      "                           Movie Name Year of Release Rating\n",
      "0                     Ship of Theseus            2012      8\n",
      "1                              Iruvar            1997    8.5\n",
      "2                     Kaagaz Ke Phool            1959    7.8\n",
      "3   Lagaan: Once Upon a Time in India            2001    8.1\n",
      "4                     Pather Panchali            1955    8.3\n",
      "..                                ...             ...    ...\n",
      "95                        Apur Sansar            1959    8.5\n",
      "96                        Kanchivaram            2008    8.1\n",
      "97                    Monsoon Wedding            2001    7.3\n",
      "98                              Black            2005    8.2\n",
      "99                            Deewaar            1975      8\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# getting input\n",
    "# the link is https://www.imdb.com/list/ls056092300/\n",
    "\n",
    "url = input(\"Enter the IMDB URL: \")\n",
    "a = imdb(url)\n",
    "\n",
    "# printing the output\n",
    "print(a)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02088fa",
   "metadata": {},
   "source": [
    "# 4. Write a python program to scrape product name, price and discounts from https://meesho.com/bags-ladies/pl/p7vbp ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4aa00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8bea3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meesho(url):\n",
    "    p = requests.get(url)\n",
    "    bs = BeautifulSoup(p.content)\n",
    "    \n",
    "    \n",
    "    name=[]\n",
    "    for i in bs.find_all(\"p\",class_=\"Text__StyledText-sc-oo0kvp-0 cPgaBh NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 hofZGw NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 hofZGw\"):\n",
    "        name.append(i.text)\n",
    "        \n",
    "        \n",
    "    original_price=[]\n",
    "    for i in bs.find_all(\"p\",class_=\"Text__StyledText-sc-oo0kvp-0 hgHnkG Paragraph__StyledParagraphBody2StrikeThrough-sc-69qp0d-0 coIjqc Paragraph__StyledParagraphBody2StrikeThrough-sc-69qp0d-0 coIjqc\"):\n",
    "        original_price.append(i.text.replace(\"₹\",\"\"))\n",
    "        \n",
    "    discount_per = []\n",
    "    for i in bs.find_all(\"span\",class_=\"Text__StyledText-sc-oo0kvp-0 cZvGTZ\"):\n",
    "        discount_per.append(i.text.replace(\"off\",\"\"))\n",
    "        \n",
    "        \n",
    "    final_price=[] \n",
    "    for i in bs.find_all(\"h5\",class_=\"Text__StyledText-sc-oo0kvp-0 dLSsNI\"):\n",
    "        final_price.append(i.text.replace(\"₹\",\"\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "    output = pd.DataFrame({\"Product Name\":name,\n",
    "                           \"Original Price\":original_price,\"Discount Percentage\":discount_per,\n",
    "                           \"Final Price after Discount\":final_price})  \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c04fa36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Meesho url: https://meesho.com/bags-ladies/pl/p7vbp\n",
      "                                         Product Name Original Price  \\\n",
      "0                     Graceful Stylish Women Handbags            484   \n",
      "1                        Classic Fancy Women Handbags            349   \n",
      "2                      Classic Stylish Women Handbags            384   \n",
      "3                   Elegant Attractive Women Handbags            436   \n",
      "4                  Voguish Fashionable Women Handbags            396   \n",
      "5                    Ravishing Stylish Women Handbags            470   \n",
      "6                      Elegant Stylish Women Handbags            484   \n",
      "7                     Voguish Alluring Women Handbags            684   \n",
      "8                     Classic Alluring Women Handbags            284   \n",
      "9                        Elite Stylish Women Handbags            479   \n",
      "10                     Graceful Classy Women Handbags            422   \n",
      "11                      Voguish Classy Women Handbags            404   \n",
      "12                 Classic Fashionable Women Handbags            107   \n",
      "13                  Trendy Fashionable Women Handbags            484   \n",
      "14                  Elegant Attractive Women Handbags            474   \n",
      "15                     Classic Stylish Women Handbags            433   \n",
      "16                       Elite Stylish Women Handbags            499   \n",
      "17                    Trendy Versatile Women Handbags             85   \n",
      "18  Ameyson Attractive Women Jute Printed Lunch Ti...            206   \n",
      "19                  Ravishing Alluring Women Handbags            815   \n",
      "\n",
      "   Discount Percentage Final Price after Discount  \n",
      "0                 21%                         384  \n",
      "1                 29%                         249  \n",
      "2                 26%                         284  \n",
      "3                 23%                         336  \n",
      "4                 25%                         296  \n",
      "5                 21%                         370  \n",
      "6                 21%                         384  \n",
      "7                 15%                         584  \n",
      "8                 30%                         199  \n",
      "9                 21%                         379  \n",
      "10                24%                         322  \n",
      "11                25%                         304  \n",
      "12                30%                          75  \n",
      "13                21%                         384  \n",
      "14                21%                         374  \n",
      "15                23%                         333  \n",
      "16                20%                         399  \n",
      "17                29%                          60  \n",
      "18                30%                         145  \n",
      "19                12%                         715  \n"
     ]
    }
   ],
   "source": [
    "# getting input\n",
    "# the link is https://meesho.com/bags-ladies/pl/p7vbp\n",
    "\n",
    "url = input(\"Enter the Meesho url: \")\n",
    "a = meesho(url)\n",
    "\n",
    "# printing the output\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ab42b",
   "metadata": {},
   "source": [
    "# 5.Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a396af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f766aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function\n",
    "def icc(url):\n",
    "    page = requests.get(url)\n",
    "    bs = BeautifulSoup(page.content)\n",
    "    table = bs.find(\"table\",class_=\"table\")\n",
    "# getting the table header names\n",
    "    header=[]\n",
    "    for h in table.find_all(\"th\"):\n",
    "        header.append(h.text.strip().split()[0])\n",
    "        \n",
    "# assigning the table header names to empty data frame        \n",
    "        \n",
    "    df = pd.DataFrame(columns=header)    \n",
    "        \n",
    "        \n",
    "    for r in table.find_all(\"tr\")[1:]:\n",
    "        data = r.find_all(\"td\")\n",
    "        row_data = [d.text.strip().replace(\"\\n\",\"\") for d in data]\n",
    "        le = len(df)\n",
    "        df.loc[le] = row_data\n",
    "        \n",
    "# getting the first 10 data in another data frame  \n",
    "\n",
    "    df1 = df.head(10)\n",
    "        \n",
    "    return df1    \n",
    "             \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6f61fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the icc url: https://www.icc-cricket.com/rankings/mens/team-rankings/odi\n",
      "  Pos            Team Matches Points Rating\n",
      "0   1   New ZealandNZ      18  2,185    121\n",
      "1   2      EnglandENG      32  3,793    119\n",
      "2   3    AustraliaAUS      29  3,387    117\n",
      "3   4        IndiaIND      38  4,162    110\n",
      "4   5  South AfricaSA      31  3,167    102\n",
      "5   6   BangladeshBAN      36  3,350     93\n",
      "6   7     PakistanPAK      28  2,590     93\n",
      "7   8     Sri LankaSL      35  2,835     81\n",
      "8   9   West IndiesWI      36  2,788     77\n",
      "9  10  AfghanistanAFG      23  1,562     68\n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.icc-cricket.com/rankings/mens/team-rankings/odi\n",
    "url = input(\"enter the icc url: \")\n",
    "# calling the function\n",
    "a = icc(url)\n",
    "# printing the data frame output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63953f5",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team and rating.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25bac8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90fa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function\n",
    "def icc(url):\n",
    "    page = requests.get(url)\n",
    "    bs = BeautifulSoup(page.content)\n",
    "    table = bs.find(\"table\",class_=\"table rankings-table\")\n",
    "# getting the table header names\n",
    "    header=[]\n",
    "    for h in table.find_all(\"th\"):\n",
    "        header.append(h.text)\n",
    "        \n",
    "# assigning the table header names to empty data frame        \n",
    "        \n",
    "    df = pd.DataFrame(columns=header)    \n",
    "        \n",
    "        \n",
    "    for r in table.find_all(\"tr\")[1:]:\n",
    "        data = r.find_all(\"td\")\n",
    "        row_data = [d.text.strip().replace(\"\\n\",\"\") for d in data]\n",
    "        le = len(df)\n",
    "        df.loc[le] = row_data\n",
    "\n",
    "# getting the first 10 data in another data frame  \n",
    "\n",
    "    df1 = df.head(10)\n",
    "        \n",
    "    return df1    \n",
    "             \n",
    "  \n",
    "             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af9ba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the icc url: https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\n",
      "                                                 Pos                 Player  \\\n",
      "0                       1                        (0)             Babar Azam   \n",
      "1               2                                (0)            Virat Kohli   \n",
      "2               3                                (0)            Ross Taylor   \n",
      "3               4                                (0)           Rohit Sharma   \n",
      "4               5                                (0)        Quinton de Kock   \n",
      "5  6                                (1)This playe...         Jonny Bairstow   \n",
      "6  7                                (1)This playe...            Aaron Finch   \n",
      "7               8                                (0)  Rassie van der Dussen   \n",
      "8               9                                (0)           David Warner   \n",
      "9  10                                (2)This play...            Imam-ul-Haq   \n",
      "\n",
      "  Team Rating              Career Best Rating  \n",
      "0  PAK    872       873 v England, 13/07/2021  \n",
      "1  IND    811       911 v England, 12/07/2018  \n",
      "2   NZ    794    841 v Bangladesh, 05/06/2019  \n",
      "3  IND    791     885 v Sri Lanka, 06/07/2019  \n",
      "4   SA    789     813 v Sri Lanka, 10/03/2019  \n",
      "5  ENG    775         796 v India, 26/03/2021  \n",
      "6  AUS    771       798 v England, 25/06/2019  \n",
      "7   SA    769    776 v Bangladesh, 20/03/2022  \n",
      "8  AUS    758      880 v Pakistan, 26/01/2017  \n",
      "9  PAK    746  768 v South Africa, 02/04/2021  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\n",
    "url = input(\"enter the icc url: \")\n",
    "# calling the function\n",
    "a = icc(url)\n",
    "# printing the data frame output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7debbfad",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b6e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a57cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function\n",
    "def icc(url):\n",
    "    page = requests.get(url)\n",
    "    bs = BeautifulSoup(page.content)\n",
    "    table = bs.find(\"table\",class_=\"table rankings-table\")\n",
    "# getting the table header names\n",
    "    header=[]\n",
    "    for h in table.find_all(\"th\"):\n",
    "        header.append(h.text)\n",
    "        \n",
    "# assigning the table header names to empty data frame        \n",
    "        \n",
    "    df = pd.DataFrame(columns=header)    \n",
    "        \n",
    "        \n",
    "    for r in table.find_all(\"tr\")[1:]:\n",
    "        data = r.find_all(\"td\")\n",
    "        row_data = [d.text.strip().replace(\"\\n\",\"\") for d in data]\n",
    "        le = len(df)\n",
    "        df.loc[le] = row_data\n",
    "        \n",
    "# getting the first 10 data in another data frame  \n",
    "\n",
    "    df1 = df.head(10)\n",
    "        \n",
    "    return df1    \n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0fcb5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the icc url: https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\n",
      "                                                 Pos            Player Team  \\\n",
      "0                       1                        (0)       Trent Boult   NZ   \n",
      "1               2                                (0)    Josh Hazlewood  AUS   \n",
      "2               3                                (0)      Chris Woakes  ENG   \n",
      "3               4                                (0)        Matt Henry   NZ   \n",
      "4               5                                (0)  Mujeeb Ur Rahman  AFG   \n",
      "5               6                                (0)    Jasprit Bumrah  IND   \n",
      "6               7                                (0)      Mehedi Hasan  BAN   \n",
      "7               8                                (0)   Shakib Al Hasan  BAN   \n",
      "8  9                                (6)This playe...        Adam Zampa  AUS   \n",
      "9              10                                (0)       Rashid Khan  AFG   \n",
      "\n",
      "  Rating             Career Best Rating  \n",
      "0    733  770 v West Indies, 22/06/2019  \n",
      "1    705      733 v England, 26/01/2018  \n",
      "2    700    711 v Sri Lanka, 04/07/2021  \n",
      "3    687   691 v Bangladesh, 26/03/2021  \n",
      "4    681      712 v Ireland, 24/01/2021  \n",
      "5    679  841 v West Indies, 01/11/2018  \n",
      "6    661    725 v Sri Lanka, 25/05/2021  \n",
      "7    657     717 v Zimbabwe, 05/11/2009  \n",
      "8    650     650 v Pakistan, 29/03/2022  \n",
      "9    650     806 v Pakistan, 21/09/2018  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\n",
    "url = input(\"enter the icc url: \")\n",
    "# calling the function\n",
    "a = icc(url)\n",
    "# printing the data frame output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c3cec",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9df5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7820b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function\n",
    "def icc(url):\n",
    "    page = requests.get(url)\n",
    "    bs = BeautifulSoup(page.content)\n",
    "    table = bs.find(\"table\",class_=\"table\")\n",
    "# getting the table header names\n",
    "    header=[]\n",
    "    for h in table.find_all(\"th\"):\n",
    "        header.append(h.text.strip().split()[0])\n",
    "        \n",
    "# assigning the table header names to empty data frame        \n",
    "        \n",
    "    df = pd.DataFrame(columns=header)    \n",
    "        \n",
    "        \n",
    "    for r in table.find_all(\"tr\")[1:]:\n",
    "        data = r.find_all(\"td\")\n",
    "        row_data = [d.text.strip().replace(\"\\n\",\"\") for d in data]\n",
    "        le = len(df)\n",
    "        df.loc[le] = row_data  \n",
    " \n",
    "    \n",
    "# getting the first 10 data in another data frame  \n",
    "\n",
    "    df1 = df.head(10)\n",
    "        \n",
    "    return df1    \n",
    "                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dca0790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the icc url: https://www.icc-cricket.com/rankings/womens/team-rankings/odi\n",
      "  Pos            Team Matches Points Rating\n",
      "0   1    AustraliaAUS      28  4,663    167\n",
      "1   2  South AfricaSA      28  3,504    125\n",
      "2   3      EnglandENG      29  3,425    118\n",
      "3   4        IndiaIND      29  2,890    100\n",
      "4   5   New ZealandNZ      31  3,018     97\n",
      "5   6   West IndiesWI      28  2,478     89\n",
      "6   7   BangladeshBAN      12    935     78\n",
      "7   8     PakistanPAK      26  1,753     67\n",
      "8   9      IrelandIRE       5    240     48\n",
      "9  10     Sri LankaSL       5    233     47\n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.icc-cricket.com/rankings/womens/team-rankings/odi\n",
    "url = input(\"enter the icc url: \")\n",
    "# calling the function\n",
    "a = icc(url)\n",
    "# printing the data frame output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0534d0",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b23e77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function\n",
    "def icc(url):\n",
    "    page = requests.get(url)\n",
    "    bs = BeautifulSoup(page.content)\n",
    "    table = bs.find(\"table\",class_=\"table rankings-table\")\n",
    "# getting the table header names\n",
    "    header=[]\n",
    "    for h in table.find_all(\"th\"):\n",
    "        header.append(h.text)\n",
    "        \n",
    "# assigning the table header names to empty data frame        \n",
    "        \n",
    "    df = pd.DataFrame(columns=header)    \n",
    "        \n",
    "        \n",
    "    for r in table.find_all(\"tr\")[1:]:\n",
    "        data = r.find_all(\"td\")\n",
    "        row_data = [d.text.strip().replace(\"\\n\",\"\") for d in data]\n",
    "        le = len(df)\n",
    "        df.loc[le] = row_data\n",
    "        \n",
    "        \n",
    "# getting the first 10 data in another data frame  \n",
    "\n",
    "    df1 = df.head(10)\n",
    "        \n",
    "    return df1    \n",
    "                         \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894f23da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the icc url: https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\n",
      "                                                 Pos             Player Team  \\\n",
      "0  1                        (1)This player has mo...    Laura Wolvaardt   SA   \n",
      "1  2                                (1)This playe...        Beth Mooney  AUS   \n",
      "2               3                                (0)        Meg Lanning  AUS   \n",
      "3               4                                (0)     Natalie Sciver  ENG   \n",
      "4               5                                (0)       Alyssa Healy  AUS   \n",
      "5  6                                (3)This playe...        Mithali Raj  IND   \n",
      "6               7                                (0)     Rachael Haynes  AUS   \n",
      "7  8                                (2)This playe...     Tammy Beaumont  ENG   \n",
      "8  9                                (1)This playe...  Amy Satterthwaite   NZ   \n",
      "9              10                                (0)    Smriti Mandhana  IND   \n",
      "\n",
      "  Rating             Career Best Rating  \n",
      "0    740    741 v Australia, 22/03/2022  \n",
      "1    726      734 v England, 03/02/2022  \n",
      "2    718  834 v New Zealand, 24/02/2016  \n",
      "3    705        712 v India, 25/02/2019  \n",
      "4    703        776 v India, 21/09/2021  \n",
      "5    686    839 v Australia, 24/12/2004  \n",
      "6    684  713 v West Indies, 15/03/2022  \n",
      "7    682        791 v India, 27/06/2021  \n",
      "8    681    756 v Australia, 02/03/2017  \n",
      "9    669      797 v England, 28/02/2019  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\n",
    "url = input(\"enter the icc url: \")\n",
    "# calling the function\n",
    "a = icc(url)\n",
    "# printing the data frame output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297c694",
   "metadata": {},
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63a20a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "830327c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function\n",
    "def icc(url):\n",
    "    page = requests.get(url)\n",
    "    bs = BeautifulSoup(page.content)\n",
    "    table = bs.find(\"table\",class_=\"table rankings-table\")\n",
    "# getting the table header names\n",
    "    header=[]\n",
    "    for h in table.find_all(\"th\"):\n",
    "        header.append(h.text)\n",
    "        \n",
    "# assigning the table header names to empty data frame        \n",
    "        \n",
    "    df = pd.DataFrame(columns=header)    \n",
    "        \n",
    "        \n",
    "    for r in table.find_all(\"tr\")[1:]:\n",
    "        data = r.find_all(\"td\")\n",
    "        row_data = [d.text.strip().replace(\"\\n\",\"\") for d in data]\n",
    "        le = len(df)\n",
    "        df.loc[le] = row_data\n",
    "        \n",
    "            \n",
    "# getting the first 10 data in another data frame  \n",
    "\n",
    "    df1 = df.head(10)\n",
    "        \n",
    "    return df1    \n",
    "                         \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d1a55f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the icc url: https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\n",
      "                                                 Pos            Player Team  \\\n",
      "0                       1                        (0)      Ellyse Perry  AUS   \n",
      "1               2                                (0)    Natalie Sciver  ENG   \n",
      "2               3                                (0)    Marizanne Kapp   SA   \n",
      "3               4                                (0)   Hayley Matthews   WI   \n",
      "4               5                                (0)       Amelia Kerr   NZ   \n",
      "5               6                                (0)  Ashleigh Gardner  AUS   \n",
      "6               7                                (0)     Deepti Sharma  IND   \n",
      "7               8                                (0)     Jess Jonassen  AUS   \n",
      "8  9                                (1)This playe...   Katherine Brunt  ENG   \n",
      "9  10                                (1)This play...    Jhulan Goswami  IND   \n",
      "\n",
      "  Rating              Career Best Rating  \n",
      "0    404   548 v West Indies, 11/09/2019  \n",
      "1    376   391 v New Zealand, 16/09/2021  \n",
      "2    359   419 v West Indies, 10/09/2021  \n",
      "3    340         365 v India, 12/03/2022  \n",
      "4    335  339 v South Africa, 17/03/2022  \n",
      "5    278    278 v Bangladesh, 25/03/2022  \n",
      "6    249  397 v South Africa, 09/10/2019  \n",
      "7    246   308 v West Indies, 11/09/2019  \n",
      "8    239     296 v Australia, 03/02/2022  \n",
      "9    217     308 v Australia, 02/02/2016  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\n",
    "url = input(\"enter the icc url: \")\n",
    "# calling the function\n",
    "a = icc(url)\n",
    "# printing the data frame output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9a3a8",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape details of all the posts from coreyms.com. Scrape the heading, date, content\n",
    " # and the code for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74d63565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from requests import get as gt\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8ad9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping heading\n",
    "heading=[]\n",
    "time=[]\n",
    "content=[]\n",
    "link=[]\n",
    "\n",
    "def coreyms(url):\n",
    "    \n",
    "    p = gt(url)\n",
    "    b = bs(p.content)\n",
    "    for h in b.find_all(\"a\",class_=\"entry-title-link\"):\n",
    "        heading.append(h.text)\n",
    "        \n",
    "    for h in b.find_all(\"time\",class_=\"entry-time\"):\n",
    "        time.append(h.text)  \n",
    "        \n",
    "        \n",
    "    for h in b.find_all(\"div\",class_=\"entry-content\"):\n",
    "        content.append(h.text.replace(\"\\n\",\"\")) \n",
    "        \n",
    "        \n",
    "    for h in b.find_all(\"iframe\",class_=\"youtube-player\"):\n",
    "        link.append(h[\"src\"])    \n",
    "    \n",
    "    \n",
    "    heading.pop(4)\n",
    "    time.pop(4)\n",
    "    content.pop(4)\n",
    "    \n",
    "    df = pd.DataFrame({\"Heading\":heading,\"Time\":time,\"Content\":content,\"Link\":link})\n",
    "    \n",
    "    df.to_csv(\"coreyms.csv\")\n",
    "    \n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57a6e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter coreyms url: https://coreyms.com/\n",
      "                                             Heading                Time  \\\n",
      "0  Python Tutorial: Zip Files – Creating and Extr...   November 19, 2019   \n",
      "1  Python Data Science Tutorial: Analyzing the 20...    October 17, 2019   \n",
      "2  Python Multiprocessing Tutorial: Run Code in P...  September 21, 2019   \n",
      "3  Python Threading Tutorial: Run Code Concurrent...  September 12, 2019   \n",
      "4  Python Quick Tip: The Difference Between “==” ...      August 6, 2019   \n",
      "5  Python Tutorial: Calling External Commands Usi...       July 24, 2019   \n",
      "6  Visual Studio Code (Windows) – Setting up a Py...         May 1, 2019   \n",
      "7  Visual Studio Code (Mac) – Setting up a Python...         May 1, 2019   \n",
      "8  Clarifying the Issues with Mutable Default Arg...      April 24, 2019   \n",
      "\n",
      "                                             Content  \\\n",
      "0  In this video, we will be learning how to crea...   \n",
      "1  In this Python Programming video, we will be l...   \n",
      "2  In this Python Programming video, we will be l...   \n",
      "3  In this Python Programming video, we will be l...   \n",
      "4  In this Python Programming Tutorial, we will b...   \n",
      "5  In this Python Programming Tutorial, we will b...   \n",
      "6  In this Python Programming Tutorial, we will b...   \n",
      "7  In this Python Programming Tutorial, we will b...   \n",
      "8  In this Python Programming Tutorial, we will b...   \n",
      "\n",
      "                                                Link  \n",
      "0  https://www.youtube.com/embed/z0gguhEmWiY?vers...  \n",
      "1  https://www.youtube.com/embed/_P7X8tMplsw?vers...  \n",
      "2  https://www.youtube.com/embed/fKl2JW_qrso?vers...  \n",
      "3  https://www.youtube.com/embed/IEEhzQoKtQU?vers...  \n",
      "4  https://www.youtube.com/embed/mO_dS3rXDIs?vers...  \n",
      "5  https://www.youtube.com/embed/2Fp1N6dof0Y?vers...  \n",
      "6  https://www.youtube.com/embed/-nh9rCzPJ20?vers...  \n",
      "7  https://www.youtube.com/embed/06I63_p-2A4?vers...  \n",
      "8  https://www.youtube.com/embed/_JGmemuINww?vers...  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://coreyms.com/\n",
    "\n",
    "url = input(\"enter coreyms url: \")\n",
    "\n",
    "# calling the function\n",
    "a = coreyms(url)\n",
    "\n",
    "# printing output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9244ee",
   "metadata": {},
   "source": [
    "# 8) Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61fb5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from requests import get as gt\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79acf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nobroker(url):\n",
    "    \n",
    "    \n",
    "    house_title = []\n",
    "    location=[]\n",
    "    area=[]\n",
    "    emi=[]\n",
    "    price=[]\n",
    "    \n",
    "    p = gt(url)\n",
    "    b = bs(p.content)\n",
    "\n",
    "\n",
    "    for i in b.find_all('span',class_=\"overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full\"):\n",
    "        house_title.append(i.text)\n",
    "     \n",
    "    for i in b.find_all('div',class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95\"):\n",
    "        location.append(i.text)\n",
    "    \n",
    "\n",
    "    for i in b.find_all('div',class_=\"p-1.5p flex border-b border-b-solid border-cardbordercolor tp:py-1p tp:px-1.5p tp:border-b-0\"):\n",
    "        area.append(i.text.split(\"₹\")[1].replace(\"sqftBuiltup\",\"\"))\n",
    "    \n",
    "    \n",
    "    for i in b.find_all('div',class_=\"p-1.5p flex border-b border-b-solid border-cardbordercolor tp:py-1p tp:px-1.5p tp:border-b-0\"):\n",
    "        emi.append(i.text.split(\"₹\")[2].replace(\"/MonthEstimated EMI\",\"\"))  \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in b.find_all('div',class_=\"p-1.5p flex border-b border-b-solid border-cardbordercolor tp:py-1p tp:px-1.5p tp:border-b-0\"):\n",
    "        price.append(i.text.split(\"₹\")[3])  \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({\"Name of House\":house_title,\"Location\":location,\"Area\":area,\"EMI\":emi,\"Price\":price})\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab1de42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the no broker url: https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d\n",
      "                                       Name of House  \\\n",
      "0  2 BHK Apartment  For Sale  In Tarang Parkway A...   \n",
      "1  2 BHK Flat  For Sale  In Kumar Ashraya Apartme...   \n",
      "2  4+ BHK In Independent House  For Sale  In Raja...   \n",
      "3  4+ BHK In Independent House  For Sale  In Jaya...   \n",
      "4  2 BHK Apartment  For Sale  In Blueberry Apartm...   \n",
      "\n",
      "                                            Location     Area        EMI  \\\n",
      "0  Tarang Parkway Apartment  2nd Main Rd, Shivana...   1,200      57,314   \n",
      "1   Kumar Ashraya Apartments 194, 9th Cross Rd 2n...     900      37,827   \n",
      "2  Independent House,  6th Cross road,9th main ro...   1,320      63,045   \n",
      "3  Independent House, TMC Layout behind rajalaxmi...   2,800   2.21 Lacs   \n",
      "4  Blueberry Apartment  13th E Main Rd, Channakes...   1,074      87,691   \n",
      "\n",
      "         Price  \n",
      "0      1 Crore  \n",
      "1      66 Lacs  \n",
      "2   1.1 Crores  \n",
      "3  3.85 Crores  \n",
      "4  1.53 Crores  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d\n",
    "url = input(\"enter the no broker url: \")\n",
    "\n",
    "# calling the function\n",
    "a = nobroker(url)\n",
    "\n",
    "# printing the output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a55dea",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location\n",
    "# iv) Ratings\n",
    "# v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1c5d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from requests import get as gt\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7802b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dine(url):\n",
    "    name=[]\n",
    "    cuisine=[]\n",
    "    location=[]\n",
    "    rating=[]\n",
    "    link=[]\n",
    "    p = gt(url)\n",
    "    b = bs(p.content)\n",
    "    \n",
    "    for i in b.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "        name.append(i.text)\n",
    "        \n",
    "        \n",
    "    for i in b.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "        cuisine.append(i.text.split(\"|\")[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    for i in b.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "        location.append(i.text)\n",
    "        \n",
    "        \n",
    "    for i in b.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "        rating.append(i.text)\n",
    "        \n",
    "        \n",
    "    for i in b.find_all('img',class_=\"no-img\"):\n",
    "        link.append(i[\"data-src\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame({\"Name\":name,\"Cuisine\":cuisine,\"Location\":location,\"Rating\":rating,\"Url\":link}) \n",
    "    \n",
    "    \n",
    "    return df\n",
    "        \n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b629a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Dineout link : https://www.dineout.co.in/delhi-restaurants/buffet-special\n",
      "                                Name  \\\n",
      "0                    Castle Barbeque   \n",
      "1                    Jungle Jamboree   \n",
      "2                    Castle Barbeque   \n",
      "3                         Cafe Knosh   \n",
      "4               The Barbeque Company   \n",
      "5                        India Grill   \n",
      "6                     Delhi Barbeque   \n",
      "7   The Monarch - Bar Be Que Village   \n",
      "8                         World Cafe   \n",
      "9                  Indian Grill Room   \n",
      "10                   Mad 4 Bar B Que   \n",
      "11                       Barbeque 29   \n",
      "12                        Glasshouse   \n",
      "\n",
      "                                        Cuisine  \\\n",
      "0                         North Indian, Chinese   \n",
      "1                  North Indian, Asian, Italian   \n",
      "2                         Chinese, North Indian   \n",
      "3                          Italian, Continental   \n",
      "4                         North Indian, Chinese   \n",
      "5                         North Indian, Italian   \n",
      "6                                  North Indian   \n",
      "7                         North Indian, Chinese   \n",
      "8            North Indian, Chinese, Continental   \n",
      "9                         North Indian, Mughlai   \n",
      "10                                 North Indian   \n",
      "11   North Indian, Mughlai, Desserts, Beverages   \n",
      "12        European, Italian, Asian, Continental   \n",
      "\n",
      "                                             Location Rating  \\\n",
      "0                      Connaught Place, Central Delhi    3.5   \n",
      "1              3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
      "2              Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
      "3   The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
      "4                  Gardens Galleria,Sector 38A, Noida      4   \n",
      "5                Hilton Garden Inn,Saket, South Delhi    3.9   \n",
      "6      Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
      "7   Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.9   \n",
      "8    Vibe by The Lalit Traveller,Sector 35, Faridabad    4.2   \n",
      "9    Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
      "10                               Sector 29, Faridabad    3.6   \n",
      "11                                     NIT, Faridabad    4.2   \n",
      "12  DoubleTree By Hilton Gurugram Baani Square,Sec...      4   \n",
      "\n",
      "                                                  Url  \n",
      "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.dineout.co.in/delhi-restaurants/buffet-special\n",
    "\n",
    "url = input(\"Enter Dineout link : \")\n",
    "\n",
    "# calling the function\n",
    "\n",
    "a = dine(url)\n",
    "\n",
    "# printing the output\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c772716",
   "metadata": {},
   "source": [
    "# 10.Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/women-tshirts?ga_q=tshirts ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a31d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from requests import get as gt\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96a462d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function\n",
    "def bewak(li):\n",
    "    p = gt(li)\n",
    "    b = bs(p.content)\n",
    "    name = []\n",
    "    price = []\n",
    "    url = []\n",
    "    nc = 0\n",
    "    pc = 0\n",
    "    uc=0\n",
    "    \n",
    "    for i in b.find_all('div',class_=\"productCardDetail\"):\n",
    "        if nc == 10:\n",
    "            break\n",
    "        name.append(i.text.split(\"₹\")[0])\n",
    "        nc +=1\n",
    "        \n",
    "        \n",
    "    for i in b.find_all('span',class_=\"discountedPriceText\"):\n",
    "        if pc ==10:\n",
    "            break\n",
    "        price.append(i.text.replace(\"₹\",\"\"))\n",
    "        pc +=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in b.find_all('img',class_=\"productImgTag\"):\n",
    "        if uc ==10:\n",
    "            break\n",
    "        url.append(i[\"src\"])\n",
    "        uc +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame({\"Product Name\":name,\"Price\":price,\"Link\":url})\n",
    "    \n",
    "    \n",
    "    return df    \n",
    "        \n",
    "    \n",
    "    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2266061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the bewakoof link: https://www.bewakoof.com/women-clothing/category-top--category-t-shirt\n",
      "                                        Product Name Price  \\\n",
      "0         White Stripes Side Panel Boyfriend T-shirt   349   \n",
      "1         Brush Stroke Whatever 3/4th Sleeve T-Shirt   249   \n",
      "2  Busy Doin Nothing Half Sleeve T-Shirt (DL)-Bab...   299   \n",
      "3       Wave Reflect Round Neck 3/4th Sleeve T-Shirt   249   \n",
      "4    Mulan Face Round Neck 3/4th Sleeve T-Shirt (DL)   249   \n",
      "5  Absolutely Awesome Bunny Round Neck 3/4th Slee...   249   \n",
      "6  Food Before Dudes Half Sleeve T-Shirt (DL) Bol...   249   \n",
      "7                   Pineapple 3/4 Sleeve AOP T-Shirt   299   \n",
      "8  Women's Round Neck 3/4 Sleeve Combo T-Shirts P...   299   \n",
      "9          Women's Black & White Striped Blouson Top   499   \n",
      "\n",
      "                                                Link  \n",
      "0  https://images.bewakoof.com/t320/white-stripes...  \n",
      "1  https://images.bewakoof.com/t320/brush-stroke-...  \n",
      "2  https://images.bewakoof.com/t320/busy-doin-not...  \n",
      "3  https://images.bewakoof.com/t320/wave-reflect-...  \n",
      "4  https://images.bewakoof.com/t320/mulan-face-ro...  \n",
      "5  https://images.bewakoof.com/t320/absolutely-aw...  \n",
      "6  https://images.bewakoof.com/t320/food-before-d...  \n",
      "7  https://images.bewakoof.com/t320/pineapple-3-4...  \n",
      "8  https://images.bewakoof.com/t320/women-s-round...  \n",
      "9  https://images.bewakoof.com/t320/style-quotien...  \n"
     ]
    }
   ],
   "source": [
    "# getting input from user\n",
    "# the link is https://www.bewakoof.com/women-clothing/category-top--category-t-shirt\n",
    "li = input(\"enter the bewakoof link: \")\n",
    "# calling the function\n",
    "a = bewak(li)\n",
    "# printing output\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5aa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93047e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
